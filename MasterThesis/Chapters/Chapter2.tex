% Chapter 2

\chapter{On-Stack Replacement} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter2} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------
\section{Overview}
\subsection{Definition}
%Replace some portion of code while it is executing 
%Used to optimize code that is running 
%Used to undo an invalid optimization of the code that is running
On-Stack replacement (OSR) is a set of techniques that consist in dynamically transferring the execution, at run time, between different pieces of code.
The action of transferring the execution to another code artefact is called an OSR transition.\\

On-Stack replacement can be viewed, at a high level, as a mechanism that allows to transform the currently executing code, into another version of itself.
This transformation mechanism has been used to allow the bi-directional transition between different levels of code optimizations.
We can therefore reduce it to two main purposes: transforming an executing piece of code into a more optimized version of itself, and undoing transformations that were previously performed.
While similar, these two types of transformation have very different goals.\\

In several virtual machines (CITE PAPERS), some of which will be presented in (REFERENCE), On-Stack replacement has been used to improve the performance of long running functions.
When the VM identifies a piece of code as being "hot", i.e., it hogs the execution, it suspends its execution, recompiles it to a higher level of optimization, and transfers the execution to the newly generated version of the function.
This differs from a simple Just-In-Time (JIT) compiler, since the recompilation takes place during the execution of the function, rather than just before its execution.
%TODO reformulate
However, both techniques rely on run time profiling data to uncover new optimization opportunities.
In this case, OSR is used to improve performance.\\

On-Stack replacement allows a compiler to perform speculative transformations.
Some optimizations rely on assumptions that are not bound to hold during the entire execution of a program.
A simple example is function inlining in an environment where functions can be redefined at any time.
A regular and correct compiler would not allow to inline a function that might be modified during the execution.
The OSR mechanism, on the other hand, enables to perform such an optimization.
Whenever the assumption fails, i.e., the function is redefined, the OSR mechanism will enable to transfer the execution to a corresponding piece of code where the inlining has not been performed.
In this case, OSR is used to preserve correctness.\\

On-Stack replacement is a powerful technique, that can be used to either improve performance, or enable speculative transformations of the code while preserving correctness.
In the next subsection, we present the historical origins of On-Stack replacement and detail its most interesting features.\\ %TODO don't like the word feature

\subsection{Why is OSR interesting?}
%The optimization case
     %wait until enough profiling information is gathered to make some new assumptions and improve code quality
    %Some enable to have several specialised versions of the code live at the same time.
    %Chaining OSR means that we can keep optimising the code -> depends on profiler
%The deoptimization case
    %This is the real deal. optimization is not valid without this counter part. 
    %Enables even more agressive code specialisation. Being able to undo means that we can have virtually any assumption and just revert back to a safe version if it fails. 
    %The real difference: optimization is for performance, deoptimazation is for correctness.
    
This section highlights the benefits that On-Stack replacement enables.
We divide them into two separate cases: OSR with regards to optimization, and OSR for deoptimization.\\

On-Stack replacement increases the power of dynamic compilation.
OSR enables to differ compilation further in the future than dynamic compilation techniques such as Just-In time (JIT) compilation.
A function can be recompile while it is executing.
This enables more aggressive adaptative compilation, i.e., by delaying the point in time when the recompilation is performed, OSR enables to gather more information about the current execution profile. These information can then be used to produce higher quality compiled code, displaying better performances.

For dynamic languages, code specialization is the most efficient technique to improve performances (IS THAT TRUE? FIND AND CITE).
Code specialization consists in tuning the code to better fit a particular use of the code, hence yielding better performances.
Specialization can be viewed as a mechanism relying on the versioning of some piece of code.
One of the main challenges is to identify which version better fits the current execution need.
This requires to gather enough profiling information, some of which might not be available until some portion of the code is executed multiple times.

OSR, coupled with an efficient compiler to generate and keep track of specialized functions, enables to uncover new opportunities to fine tune a portion of code.
While techniques like JIT compilation can generate specialized code at a function level, i.e., before the execution of a function, OSR enables to make such tuning while a function is running.
For example, in the case of a long running loop inside a function, JIT techniques would need to wait until the function is called anew to improve its run time performance by recompiling it. 
OSR, on the other hand, gives the compiler the means to make such improvements earlier, hence yielding a better overall performance for the executing program.

OSR is a tool that enables the compiler to recompile and optimize at almost any time during the execution of a program.
A clever compiler can perform iterative recompilation of the code in order to improve the quality of the generated compiled artefact.
OSR enables these iteration steps to be closer to each other and potentially converge to a better solution faster than other dynamic compilation techniques.\\

On-Stack replacement's most interesting feature is deoptimization. 
While optimization enables to increase performance, deoptimization's goal is to preserve correctness of the program that executes.
OSR allows speculative optimizations which, in turn, weakens the requirements for compiled code correctness. 
In other words, the compiler can generate aggressively optimized code. 
Virtually any assumption can be used to generate compiled code and, if the assumption fails, 
OSR enables to revert back to a safe version during the execution.\\

\section{On-Stack Replacement Mechanisms}

\subsection{The OSR Points}
%TODO do a definition case for that
%Several names for it 
%what it is: a point in the code from which we can suspend the exec and optimize the current version of the code
    %Implies that we need a "safe state”, i.e., we need to be able to find an equivalent point in the optimized version
    %portions of code that don’t have interrupts
    %Too many -> bad Too few -> bad
%Guarded vs Unguarded 
    %Guarding when there is an explicit condition that we can check locally
    %What if global/External event? 
        %Some have a map of such points and fix them whenever an external event happens (citing papers e.g., jikes)
%OSR exits 
    %What to do when the condition fails? 
        %optimization dependent 
        %for external requires to correct every callee return landing spots. 
        %Requires corresponding entry in the unoptimized version
%Examples from existing implementations 
    %MCJIt inserter and instrument passes (need a code transformer + OSR Label)
    %Jikes
The OSR mechanism is enabled at specific instruction boundaries in the user's code.
Depending on the OSR implementation, these points can be sparse, restricted to special points in the control flow, or associated with special framework dependent data.
%TODO not sure next sentence is needed.
This section presents different implementations of such points in the available literature.
In the examples of OSR implementations given so far (CITE SECTIONS), they have been called \textit{interrupt points}(CITE), \textit{OSR entries}, and \textit{OSR exits}(CITE).
In the reminder of this paper, we will refer to such points as \textit{OSR points}, as defined in Definition \ref{OSRPointDefinition}.

\begin{definition}\label{OSRPointDefinition}
An OSR point is a portion of code that belongs to the OSR instrumentation.
It is located at an instruction boundary at which the program can be suspended.
OSR points can correspond to several instructions inserted by the OSR framework and enable to switch the execution from one version of the code to another.
\end{definition}

%TODO continue this part by explaining why we need it to be safe
An OSR point has to be located at a point of the code where the state is \textit{safe}, i.e., points where the state is guaranteed to be consistent.
For example, as explained before, the SELF debugger considers points at method prologues and at the end of loop bodies.
An OSR point must be such that the state of the computation can be extracted and transferred to another version of the code from which we can resume the execution.\\

\subsubsection{Guarded \& Unguarded}

An OSR point can be guarded or unguarded.
The WebKit OSR framework (CITE) and the Jikes RVM OSR implementation(CITE) distinguish OSR points that are guarded, i.e., there is a boolean condition wrapping the execution of the point, from unguarded ones.
A guarded OSR point is such that the framework has enough information locally to test whether or not an OSR transition should be taken.
While simple to model at a high-level, a guarded OSR points presents the disadvantage of increasing the length of the OSR instrumentation, hence impacting on the overall performance of the program.
As a result, implementations likes WebKit and Jikes RVM OSR provide a second mechanism that we call unguarded OSR points.
In WebKit, an unguarded OSR point is a portion of the code that can be overwritten on some external condition, and hence instrumented to trigger an unconditional transition to another version whenever it is re-written.
It also enables to lazily define the transition, i.e., the instrumentation that enables to jump out of the current version can be added just before being executed.
TALK ABOUT IN JIKES\\

\subsubsection{OSR Entries \& OSR Exits}

There is a further distinction between OSR points that are responsible for optimising the code, and the ones that are used to unoptimize.
For that, we will use the same terminology as in WebKit and MCJit(CITE).
%TODO reformulate definitions
\begin{definition}\label{OSREntryDefinition}
An OSR entry is an OSR point that enables to replace the current version of the code with one in which we expect to have better performances.
They are used in the optimization process.
\end{definition}

\begin{definition}
An OSR exit is an OSR point that enables to exit the current version of the code when it is invalidated.
OSR exits are responsible for preserving the correctness of the program.
They are used in the deoptimization process.
\end{definition}

CONTINUE

\subsection{The Transition Mechanism}
The OSR mechanism can be narrowed down to its core functionality: allowing a transition at run time between two compiled versions of the same source code.
This transition mechanism is therefore the cornerstone of any OSR implementation.
This section strives to unify all the existing implementations under a single, general, high-level list of requirements to perform an OSR transition.
The section then provides a machine-level description of the ideal OSR implementation, before introducing a slightly higher-level description of an OSR transition.\\

INTRODUCE TERMINOLOGY!

\subsubsection{High-Level requirements for OSR transitions: transformation challenges}\label{HLREQ}
%OSR transition is a mapping
    % mapping = reconstruct the state 
        %what it means to reconstruct the state: live values
        % what if opt introduces new values ?
    %mapping = find the correct instruction to which
%An OSR transition is transparent
    %Should be triggered without the user intervention
    %Requires to deviate from regular execution path. 
%OSR transition can be biderectional

An OSR transition relies on a mapping between two program points and their respective states.
Such a mapping is possible only if all the information available at the starting site is sufficient to reconstruct the state expected at the landing site.\\

Reconstructing the state at the landing site means that all the expected live variables must be assigned the correct values to safely resume the execution in the new version of the code.
Propagating live values is, itself, a challenge and requires to keep a mapping between the live values in the source and the live values in the destination code.
Compiler optimizations might eliminate variables, fuse them, create new ones, hence preventing a one-to-one mapping between the two sets of live values. 
As a result, the OSR transition implementation must provide a function \textit{f} that takes the set of live variables and their assigned values $S_s$ from the source version \textit{s} and generates a new set of live variables mapped to their values $S_d$ for the destination version \textit{d}.

\[f: S \rightarrow S\]
\[f(S_s) = S_d\]

An OSR transition is transparent to the user, i.e., it should not be observed by the user running the program.
The OSR transition can be viewed as a mechanism that deviates the program from its current execution path in order to either improve the performances (the optimization process) or preserve correctness (the deoptimization process).
It is part of the compilation/execution framework and, aside form a performance variation, should not impact the execution of the program, e.g., a correct program should not crash because of an OSR transition.\\

An OSR transition can be either \textit{unidirectional} or \textit{bidirectional}.
In the case of a bidirectional mapping between the source and the destination version, the OSR framework must also provide a function \textit{f'} that enables to get all the live values for the source version from the destination ones.  
\[f': S \rightarrow S\]
\[f'(S_d) = S_s\]
This corresponds to the deoptimization case and presents a greater challenge than the optimization case.
Consider the following example: 
%TODO find a better presentation
\[a \leftarrow 5\]
\[b \leftarrow 6\]
\[c \leftarrow a + b\]
And the optimized version:
\[c \leftarrow 11\]
The function \textit{f'} must provide a way to re-generate \textit{a} and \textit{b} from \textit{c}. 
To this end, it must remember the steps that led to the optimized version of the code, and must be able to reverse them.
This implies a lot of metadata attached to each version of the optimized code generated by the compiler and might quickly increase the space complexity of the OSR support.
EXAMPLE VARMAP FROM JIKES!!! 

\subsubsection{The ideal transition: technical challenge}
%low-level transition description.
%hit the point
    %save the register content.
    %modify the stack to have expected values, calls made by the other function
    %put register content
    %modify program counter and resume execution
%Hard if do not have complete control over the execution platform, and still need to be able to understand machine level meaning of the code etc.
At a machine code level, an ideal OSR transition can be summarised into 5 steps: 
\begin{enumerate}
    \item Save the content of the registers.
    \item Save the local live values, i.e., the on-stack values in the current stack frame.
    \item Modify the current stack frame to introduce the expected local values, add expected stack frames or remove the stack frames that are not expected.
    \item put the correct content inside the registers.
    \item modify the program counter and resume execution.
\end{enumerate}
In order to perform such a modification of the runtime stack and registers, the OSR framework must have complete control over the machine executing the program.
This requirement is easier to satisfy if the execution is performed inside a virtual machine that enables to access and modify the execution stack and the registers.
Section \ref{OSR&VM} expands on the relation between OSR implementations and Virtual Machines.
Furthermore, as explained in \ref{HLREQ}, reconstructing a state from a set of live values is challenging. 
Performing this operation at such a low level is hard.
The compiled version of the code loses the high-level semantics specific to the language, translates some high-level operations into several sequential instructions and performs some low level ressource management (e.g., register allocation) that requires the OSR mechanism to perfectly understand how the compiled code is generated in order to enable the OSR transition at machine code level.\\

While tempting for its fine-grained control over the transition, the implementation of the OSR mechanism at a low level seems to unveil great challenges.
An alternative solution is to encapsulate the entire process at a higher level, closer to the original program semantics.

\subsubsection{Transition as a Function call}
%A function call at a high level
    %In fact, a function call is the basic way of jumping to another portion of code
    %Propagate the state
        %load it 
        %pass it as arguments to the call.
    %Single entry point to the continuation function means for each spot we need a function
    %Multiple entry points
An OSR transition can be modelled as a function call.
As explained previously, an OSR transition needs to propagate values and jump to a continuation point outside of the current execution path.
A function call allows to "modify" the program counter.
The OSR transition therefore correspond to a function call from the base function, to a \textit{continuation} function responsible for executing the new version of the code.\\

In order to propagate the current execution state, there are two obvious solutions: 
\begin{enumerate}
    \item Dumping the state in a shared memory location(e.g., as in MCJIT OSR (CITE) presented in REF), or
    \item Passing all the needed values as function call arguments to the continuation function(e.g., OSR Kit, described in REF).
\end{enumerate}

The first option implies that the \textit{from function} must dump its state into a buffer before calling the \textit{continuation function}.
The \textit{continuation function} starts its execution by loading the values it needs from the buffer.
This solution has the drawback of increasing both the \textit{from function} and the \textit{continuation function} lengths, i.e., the OSR propagation of the state has a greater impact on the performance of the execution than a simple function call cost.\\

For the second option, most of the work needed to propagate the state is performed during the compilation.
Values are passed as arguments to the call to the \textit{continuation function}.
If there is not a one-to-one mapping between the from variables and the continuation ones, some \textit{compensation} code is executed, either before the call or at the beginning of the continuation function in order to assign the correct values to the continuation local variables.
The cost of executing the compensation code is also present in the first option, i.e., both solutions have to perform the same set of transformations in order to generate the expected local values.
On the other hand, the second option does not require to access shared memory to store and load propagated values, and is therefore expected to have a smaller cost at execution.\\

The OSR transition requires to enter the continuation function in at special entry point, that corresponds to the  


\subsection{Caching vs. Generating on the Fly}
\subsubsection{One version}
->bidirectional?
\subsubsection{Multiple Versions}
\subsubsection{Discussion}
time vs. space, reusability etc.

\section{The Deoptimization case}

\subsection{Why the Deoptimization case is more interesting?}

\subsection{Where do we exit?}
\subsubsection{The Interpreter}
\subsubsection{The base version instrumented}
\subsubsection{A less optimized version}

\section{Constraints on Mechanism}
\subsection{space vs. time}
\subsection{Limits on supported optimizations}


