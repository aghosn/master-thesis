% Chapter 2

\chapter{Related Work} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter2} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

\section{On Stack Replacement, General Principle}

\subsection{Definition \& Overview}
%Replace some portion of code while it is executing 
%Used to optimise code that is running 
%Used to undo an invalid optimisation of the code that is running
On-Stack replacement (OSR) is a set of techniques that consist in dynamically transferring the execution, at run time, between different pieces of code.
The action of transferring the execution to another code artefact is called an OSR transition.\\

On-Stack replacement can be viewed, at a high level, as a mechanism that allows to transform the currently executing code, into another version of itself.
This transformation mechanism has been used to allow the bi-directional transition between different levels of code optimisations.
We can therefore reduce it to two main purposes: transforming an executing piece of code into a more optimised version of itself, and undoing transformations that were previously performed.
While similar, these two types of transformation have very different goals.\\

In several virtual machines (CITE PAPERS), some of which will be presented in (REFERENCE), On-Stack replacement has been used to improve the performance of long running functions.
When the VM identifies a piece of code as being "hot", i.e., it hogs the execution, it suspends its execution, recompiles it to a higher level of optimisation, and transfers the execution to the newly generated version of the function.
This differs from a simple Just-In-Time (JIT) compiler, since the recompilation takes place during the execution of the function, rather than just before its execution.
%TODO reformulate
However, both techniques rely on run time profiling data to uncover new optimisation opportunities.
In this case, OSR is used to improve performance.\\

On-Stack replacement allows a compiler to perform speculative transformations.
Some optimisations rely on assumptions that are not bound to hold during the entire execution of a program.
A simple example is function inlining in an environment where functions can be redefined at any time.
A regular and correct compiler would not allow to inline a function that might be modified during the execution.
The OSR mechanism, on the other hand, enables to perform such an optimisation.
Whenever the assumption fails, i.e., the function is redefined, the OSR mechanism will enable to transfer the execution to a corresponding piece of code where the inlining has not been performed.
In this case, OSR is used to preserve correctness.\\

On-Stack replacement is a powerful technique, that can be used to either improve performance, or enable speculative transformations of the code while preserving correctness.
In the next subsection, we present the historical origins of On-Stack replacement and detail its most interesting features.\\ %TODO don't like the word feature
  

\subsection{The origins: SELF debugging}
%SELF needs aggressive optimisations to have reasonable performance. 
%But that prevents from debugging
%Hence OSR enables selective deoptimization at runtime, which provides source level information 
%That makes some optimisations not available since they are hard to undo (e.g. tail %recursion elimination)
%Scope descriptors enable mapping between optimised and unoptimised, enables to keep track %of the position in the virtual call tree etc. (will be detailed). 
%Interrupt points (where, why, how)
%Function invalidation
The SELF programming language is a pure object-oriented programming language.
SELF relies on a pure message-based model of computation that, while enabling high expressiveness and rapid prototyping, impedes the languages performances(CITE from self paper).
Therefore, the language's implementation depends on a set of aggressive optimisations to achieve good performances(CITE).
SELF provides an interactive environment, based on interpreter semantics at compiled-code speed performances.\\

Providing source level code interactive debugging is hard in the presence of optimisations.
Single stepping or obtaining values for certain source level variables might not be possible.
For a language such as SELF, that heavily relies on aggressive optimisations, implementing a source code level debugger requires new techniques.\\

In (CITE Holzle), the authors came up with a new mechanism that enables to dynamically de-optimise code at specific interrupt points in order to provide source code level debugging while preserving expected behaviour (CITE from holzle).\\

In (CITE), Hölzle et al. present the main challenges encountered to provide debugging behaviors, due to the optimisations performed by the SELF compiler. 
Displaying the stack according to a source-level view is impeded by optimisations such as inlining, register allocation and copy propagation.
For example, when a function is inlined at a call spot, only a single activation frame is visible, while the source level code expects to see two of them.
Figure (FIG), taken from (CITE), provides another example of activations discordances between physical and source-level stacks.
In this figure, the source-level stack contains activations that were inlined by the compiler. For example, the activation B is inlined into A', hence disappearing from the physical stack.\\
\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{Figures/Figure1}
\decoRule
\caption[physical vs. source-level stacks]{Displaying the stack CITE}
\end{figure}


\subsection{Why is OSR interesting?}
%The optimisation case
     %wait until enough profiling information is gathered to make some new assumptions and improve code quality
    %Some enable to have several specialised versions of the code live at the same time.
    %Chaining OSR means that we can keep optimising the code
%The deoptimization case
    %This is the real deal. Optimisation is not valid without this counter part. 
    %Enables even more agressive code specialisation. Being able to undo means that we can have virtually any assumption and just revert back to a safe version if it fails. 
    %The real difference: optimisation is for performance, deoptimazation is for correctness.


\section{On Stack Replacement \& Virtual Machines}
\subsection{In Java}
%Java Hotspot 
%Graal 
%Jikes

\subsection{LLVM}
%What is LLVM? 
    %Some description of LLVM, putting the emphasis on the fact that it enables to generate native code from LLVM IR, and that many languages have an LLVM compiler (e.g. R, Ruby, Python, Matlab etc.) 
    %Provides tools and mechanism that we can reuse (e.g. low level optimisations, passes on the code etc.) 
%Why OSR on LLVM? 
    %General technique that can be profitable to several languages. 
    %Don’t need to worry about static optimisations 
    %Get portability for free
    %Examples: Webkit & McJit

%probably move that somewhere else
\section{A Description of Existing Implementations}
\subsection{The OSR points}
%Several names for it 
%what it is: a point in the code from which we can suspend the exec and optimise the current version of the code
    %Implies that we need a "safe state”, i.e., we need to be able to find an equivalent point in the optimised version
    %portions of code that don’t have interrupts
%Guarded vs Unguarded 
    %Guarding when there is an explicit condition that we can check locally
    %What if global/External event? 
        %Some have a map of such points and fix them whenever an external event happens (citing papers e.g., jikes)
%OSR exits 
    %What to do when the condition fails? 
        %Optimisation dependent 
        %for external requires to correct every callee return landing spots. 
        %Requires corresponding entry in the unoptimised version
%Examples from existing implementations 
    %MCJIt inserter and instrument passes (need a code transformer + OSR Label)
    %Jikes
\subsection{The Transition Mechanism}
%The ideal model 
    %Being able to stop execution, save the state, generate a new version, fix the state, replace the instruction pointer and resume.
%The more reasonable case of function calls 
    %Implemented as a function call 
    %In SELF: data structures on the side to keep mapping between virtual and physical (might be redundant with 2.1.2)
    %Jikes: the VARMAP that enables to do it virtually at and across OSR points, registers state etc. 
    %MCJit: Saving all live variables, instruments code to jump to correct spot in new version by instrumenting the prolog of the function + executing a special block to fix the state etc. 
    %Rome & Others (to be defined): pass everything as arguments, do a function call and instrument entry point of the function to fix the state and jump to correct continuation point.  

\subsection{Constraints and Limitations}
%Limits the types of optimisations that can be performed (e.g. tail recursion elimination in SELF) 
%Limits code motion + Mechanisms extend variable live range
%Limited spots where we can perform the transition, often limited to function pro/epilogues and loops  
    %Due to the difficulty of finding/ensuring a mapping between optimised and unoptimised otherwise. 
%The space complexity 
    %need to keep additional information around (e.g. variables that were eliminated need to be correctly re-generated when deoptimizing)
    %Instrumentation increases the code
\subsection{Generating on the Fly VS Caching}
%Caching enables to keep several versions around + saves compilation time
%On the fly: we can keep the same address for the function

\subsection{Discussion}
%The proposed classification in MCJit (several optimisation, possible deoptimization) 
%The trade off between performance gain and the compilation & instrumentation costs. 
%The paradoxal goal of finding a general technique to aggressively specialise code in particular assumption based circumstances. 
    %By being too general, we actual bring limitations to the implementation 
    %Assumption based optimisations need to be implemented for the OSR mechanism (e.g., mcjit “transformer” method) 
        %How to find a correct API that satisfies every possible optimisation that we might want to perform? 
            %Support external event
            %Support guards
%The challenge of code motion 



