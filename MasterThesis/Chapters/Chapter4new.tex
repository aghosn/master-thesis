% Chapter 4

\chapter{OSR in RJIT} % Main chapter title

\label{Chapter4New} % For referencing the chapter elsewhere

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------
\section{Overview}
\subsection{Justification \& Goals}
            
R is a programming language and software environment for statistical computing and graphics, developed by the R Foundation for Statistical Computing\cite{RURL}.
R is a dynamic, lazy, functional, object oriented programming language where the basic data type is the vector.
The dynamic features provided by the language include reflection over the environment, the ability to generate source code for unevaluated expressions and to treat text as code (e.g., functions \textit{parse} and \textit{eval}).
Even though R is considered functional, i.e., functions are first-class values, and arguments are deep copied and lazily evaluated, the language does not optimize recursions and, instead, encourages vectorized operations.\\

Although very flexible and widely used for statistical data analysis, the R programming language's implementation suffers from poor execution performances, compared to equivalent programs executed in C or Python\cite{morandat2012evaluating}.
According to \citean{morandat2012evaluating}, on the Shootout baseline benchmarks suite\cite{Shootout}, R is on average 501 times slower than C and 43 times slower than Python.
According to \cite{morandat2012evaluating}, R programs usually consume large amounts of memory, and spend an important part of the execution (e.g., on average 30\% for the Shootout benchmarks) doing memory management.
These slow downs have various causes.
For example, in R, all user data must be heap allocated and garbage collected, when languages like C allow stack allocated data.
Another R behavior that has a large impact on memory consumption is boxing.
All numeric data has to be boxed in R, leading to high overheads in programs containing an important amount of single numbers.
For example, in the Bioconductor corpus of programs\cite{Bioconductor}, 36\% of vectors allocated contained only a single number\cite{morandat2012evaluating}.
It is important to note that other R specific behaviors, such as looking up variables, matching parameters with arguments, and copy-on-write mechanisms used for call-by-value semantics have non negligible impacts on the performance of R programs\cite{morandat2012evaluating}.\\

The RJIT project\cite{Rjit} strives to improve R programs performances by providing an LLVM based JIT compiler for R.
The RJIT compiler plugs into a modified version of the GNUR interpreter and enables to JIT compile R programs into native code.
The RJIT compiler is still pretty young, only a few months old.
As a result, RJIT is currently trying to identify code behaviors that impact the execution performances, and plans to optimize code compilation to decrease their costs.
Due to R semantics, such compilation optimizations might be hard to implement.
On-stack replacement therefore appeared as an interesting solution to enable aggressive and speculative optimizations.
The goal of this master thesis is thus to prototype a flexible and extensible OSR mechanism for speculative optimizations.\\

This master thesis prototypes OSR deoptimization mechanisms in RJIT.
The RJIT OSR implementation relies on the OSR Kit\cite{OSRKit} library transition mechanism.
The OSR Kit library code is available on Github\cite{OSRKitGit}, and can be easily integrated in any LLVM project by simply copy-pasting the OSR Kit files inside of it.
Instead of reimplementing the same transition mechanisms for RJIT OSR, the choice was made to modify and extend this library.
This enables to focus on the deoptimization case which, according to Section \ref{WhyDeopt}, is the most challenging and interesting use case of on-stack replacement.
The OSR deoptimization further allows speculative optimizations, which enable the RJIT compiler to port static compilation optimizations to R, while preserving the correctness of the program being executed.\\

\subsection{OSR Kit limitations}\label{osrkitlimitations}

%While very flexible, comes short on several points
    %Open OSR, not really fitting our case.
    %Continuation style is fine for optimization, but pretty bad for deopt.
        %no replacement when exits... 
        %many clones ... 
        %As we'll see later, cloning is not sufficient ...
While very flexible, the OSR Kit\cite{OSRKit} library presents several disadvantages and exhibits costly behaviors that do not perfectly fit the deoptimization process.
This section lists the OSR Kit library's limitations with regard to OSR exits implementation.\\

The main advantage of the open OSR is to leverage profiled-guided compilation strategies to generate efficient code.
However, generating a transformation that removes optimizations is hard and does not play well with the OSR Kit instrumentation.
Such a transformation needs to take into account other optimizations that might have been performed on the code after the optimization was applied, i.e., deoptimizing requires to keep track of which transformations performed on the code depend on this optimization, and undo them.
For the OSR point, the framework relies on a StateMap that matches values in the from and the continuation functions.
That implies that, in order to generate the continuation function on the fly for the deoptimization case, one is required to 1) be able to reverse an optimization, that might have interfered with other optimizations, and 2) to generate a correct StateMap to give as input to the OSR Kit machinery.
Both of these requirements are hard to satisfy.
Finally, the deoptimization case is used to preserve correctness in the program and must be conservative. 
Performance is a soft issue, and hence leveraging profiled-guided compilation strategies is not a main objective. \\

Once an OSR exit is taken, it is more likely to be fired in subsequent calls.
For example, in the case of call site inlining, the OSR exit is fired when the inlined function is redefined.
In subsequent calls, the OSR exit will also be triggered.
As a result, if the OSR exit is an open OSR point, the framework will have to perform expensive operations to generate the correct continuation function every time the OSR exit is triggered.
In the light of these observations, we can conclude that the open OSR design does not fit well the deoptimization case.\\

The resolved OSR corresponds to our requirements for deoptimization.
The optimized version is obtained by applying a transformation on a base function.
Since OSR points cannot be tampered with, the mapping between the OSR exit and the base function is guaranteed to be preserved, regardless of the subsequent (valid) optimizations performed on the optimized function.
Another way to see this is that the OSR exit is a call to the continuation function, which takes as arguments all the values needed at the continuation function to reconstruct the computation state and continue the execution.
As long as the OSR exit call is preserved, and its arguments valid, the continuation function should be able to be executed properly.
For the overall execution to be correct, additional constraints, such as preventing instructions with side-effects from crossing OSR points, are however still required.
In the light of these observations, it makes sense to instrument the base function to generate a continuation function for the OSR exit.\\

The OSR Kit way of generating the continuation function is expensive.
In the deoptimization case, inserting a resolved OSR exit requires to 1) clone the base function in order to obtain two copies: one is kept as is, the other will be optimized, 2) generate the continuation function. 
Since the continuation function might have a different signature than the base function, the framework has to generate a clone of the body of the base function that will be instrumented with the OSR ENTRY block.
Generating two clones of the base function, in order to insert a resolved OSR exit, might be very expensive for big functions.\\

Finally, as it was the case for the open OSR, the resolved OSR does not take into account the fact that an OSR exit might be triggered at every call, once it has been fired a first time.
Although less expensive than the open OSR, a resolved OSR exit that fires at every execution adds the costs of evaluating the OSR condition, and a function call and return, compared to the base unoptimized function.
The OSR Kit framework does not provide the mechanisms required to avoid this extra cost.\\

\subsection{The RJIT compiler}
\subsubsection{General}
CITE GNUR\\
%General information
The RJIT project is an LLVM based JIT Compiler for the R programming language.
It relies on GNUR, an interpreter for R. 
The RJIT compiler takes as input a symbolic expression (SEXP), i.e., a special C structure generated by GNUR while parsing R programs.
A SEXP represents R constructs and contains the AST representation of an R element.
A SEXP has a type, and can be seen as a lisp-like cons cells, composed of two pointers, CAR and CDR.
A SEXP has many other elements, that we do not detail here.
Depending on its type, each SEXP element corresponds to a specific attribute (see examples for closures and functions below). 
The GNUR interpreter has been instrumented to call the JIT compiler before evaluating a non-compiled SEXP. 
The RJIT compiler generates native code, using the LLVM framework, that corresponds to the R function's AST it received as argument.
The native code is then wrapped into the appropriated SEXP constructs, and returned to the GNUR interpreter.
Afterwards, the GNUR interpreter executes the native code.\\ 

SMALL PICTURE OF THE FLOW LIKE FOR WEBKIT\\

\subsubsection{The RJIT compilation flow}
RJIT walks the SEXP ast and generates the corresponding LLVM IR.
Since R is a dynamic language, most R constructs need to be simulated by LLVM calls to special functions, called R intrinsics.
Intrinsic functions provide basic functionalities, such as resolving a closure from a symbol and the current environment, getting a value associated with a symbol, setting a local variable, creating a promise, or performing generic arithmetic operations.
Figure REF provides R code performing some of these standard operations, and the equivalent, simplified, LLVM IR generated for this code, with the corresponding intrinsic calls.\\

\includecode{Code/RAndLLVM.cpp}
R resolves callee of function calls at runtime, when the call is executed.
In order to simulate this behavior, the RJIT compiler needs to rely on a special instrumentation to compile calls to functions, especially the ones to functions that are not yet defined.
To that end, the JIT compiler replaces function calls with inlined cached stubs (i.e., ic stubs).
An ic stub takes as parameters the function call's arguments, a pointer to the callee extracted from the environment, and a pointer to the caller.
The ic stub, when executed, triggers the compilation of a wrapper function, call it the ic callee, that caches the resolved callee.
This wrapper function tests that the callee pointer that it gets as argument points to the same function as the one used during its own compilation.
If that is the case, it performs the call to the callee, and propagates the value returned after the call.
If the callee pointer references another function, it performs an ic stub call.
Once the ic callee function is compiled, the ic stub replaces itself in the caller with a call to the ic callee.
In order to do so, the ic stub relies on LLVM intrinsics called Stackmaps\cite{llvmStackMap} and Patchpoints\cite{llvmPatchpoints}, and on the caller's pointer it received as argument.\\

\subsubsection{The function's SEXP}
In GNUR and RJIT, a function is represented by a special SEXP. 
This SEXP has a NATIVESXP type when the function is compiled by the RJIT compiler, and a LANGSXP, i.e., a language construct, when it is not.
In the case of a compiled function, the CDR element contains a pointer to the native code of the function.
In the case of a non-compiled function, the CDR contains the AST of the function.
The CAR element, in both cases, contains the constant pool associated to the function, i.e., the list of symbols defined inside the function.
For a compiled function, the TAG element of the SEXP contains the LLVM IR of the function.\\

When the GNUR interpreter evaluates a function SEXP, it checks its SEXP type. 
If the function is compiled, it performs the call to the native code. 
If not, it calls the RJIT compiler, and executes the resulting native code.\\

In RJIT, the LLVM IR for a function always has a signature of type $T$:
$$T: (\text{SEXP}, \text{SEXP}, i32) \rightarrow \text{SEXP}.$$
where the first SEXP element is a pointer to the constant pool associated with the function, and the second SEXP a pointer to the environment.
As shown in Figure REF, the constant pool (CP) is used to access variable symbols. 
For example, line 11 corresponds to a call to the \textit{genericGetVar} intrinsic, that enables to retrieve a local variable's value from the environment.
The symbol corresponding to the variable is extracted from the CP, at index 2, and used along side the environment (i.e., \textit{rho}), to access the corresponding value.
In general, the environment is used to access variable values as well as function definitions (e.g., line 15).\\


\subsubsection{The closure's SEXP}
In R, a closure contains a function and the environment associated to it.
A closure SEXP has type CLOSXP.
The CAR of a SEXP closure is a function SEXP.
The CDR of a SEXP closure is an environment SEXP, of type ENVSXP.
A closure also contains the formals, i.e., the arguments symbols of the function.
The getFunction intrinsic, that enables to resolve a function, returns a closure.
The environment is made of cons cells, and is essential to the execution of a function.
Local variables and function argument values are set inside the environment.\\


\section{OSR Handler}
This section presents the OSR Handler, a special singleton implemented in RJIT that strives to enable efficient OSR deoptimizations. 
The OSR Handler has two main goals: 1) to mitigate the limitations of the OSR Kit\cite{OSRKit} library exposed in \ref{osrkitlimitations}, 2) to adapt the OSR Kit library to the RJIT framework.\\

This section proceeds as follow: first, it exposes the additional challenges that are inheritant to the use of the OSR Kit library in RJIT.
Then, it presents the OSR Handler implementation, and the solutions adopted for each problem encountered while enabling OSR deoptimization in RJIT.\\
 
\subsection{Additional challenges in RJIT}\label{additionalchallenges}

%Compilation flow
RJIT, and more specifically its compilation flow and the instrumentation to enable run time garbage collection and call resolution, uncovers additional challenges in the use of OSR Kit\cite{OSRKit} for the deoptimization case.\\

RJIT provides a just-in-time compiler, i.e., functions are compiled just before being evaluated.
The compilation flow can be broken down to these elementary steps: 
\begin{enumerate}
    \item The function is extracted from the R SEXP closure.
    \item If the function is not of type NATIVESXP, in other words, if the function has not been jitted before, the RJIT compiler is summoned.
    \item The function is compiled into an LLVM IR.
    \item Calls to non-intrinsic functions are compiled as inline cached stubs (ic stubs), that is, special call instructions which, at run time, resolve the callee, compile it, replace themselves with a call to an inlined cached version of the target, and make the call.
    \item Every patchpoint and safepoint, set as LLVM attributes\cite{llvmAttribute} to call instructions, is visited in order to add special instrumentation for the garbage collector and the correct execution of ic stubs.\label{jitAll}
\end{enumerate}

The final step is performed by calling the \textit{jitAll} function on the compiler instance.
All functions generated with this instance of the compiler will be instrumented during this call.
The final result of the compilation is a blotted piece of LLVM IR from which the R semantics are hard to extract.\\

In order to perform transformations based on R semantics, a fresh non-instrumented (i.e., without the safepoint and patchpoint machinery) version of the LLVM IR is needed.
A fresh non-instrumented IR can be obtained from the compiler, by (re)compiling the function, without calling step \ref{jitAll} above.
If the function was not previously compiled, this solution is the only available option.
If the function was previously compiled, but not instrumented, the LLVM IR can be extracted and used for the transformation.
If it was instrumented, the IR is unusable, and we therefore need to re-compile it.
Recompiling functions is not a satisfactory solution. 
We therefore have to find a way to avoid this case as much as possible.\\

Simply cloning the LLVM IR is not enough in RJIT.
As explained above, special instrumentation is inserted as attributes to call instructions and used for the instrumentation in step \ref{jitAll}.
In order to fully compile and execute a clone, the LLVM attributes need to be set at each of its call instructions to allow the safepoint instrumentation.
This is mandatory and, if not done properly, leads to failures during the execution.
Another problem with clones arises from the targets of function calls. 
The callee function has to be an LLVM function declared in the current module. 
If the clone comes from a different module than the one in which its compilation is being performed, the callee might not be present in the current module and triggers compilation errors. 
One last thing to take care of are the ic stubs calls.
An ic stub takes as argument a pointer to its caller.
When a function containing ic stubs is cloned, those pointers point to the original function.
This needs to be fixed if the clone is to be fully compiled and executed.
Section \ref{osrkitlimitations} states that one drawback in the OSR Kit implementation comes from the number of clones that have to be generated to insert a resolved OSR point.
In RJIT, the cost of fixing the LLVM IR, as explained in this paragraph, has to be added to the cost of cloning.\\

OSR Kit continuation functions are not compatible with ic stubs.
The ic stubs expect to receive pointers to their callers as argument.
In RJIT, every function is supposed to have the following type signature:
$$T: (\text{SEXP}, \text{SEXP}, i32) \rightarrow \text{SEXP}.$$
According to Section \ref{describeOSRKit}, the OSR Kit\cite{OSRKit} library modifies the continuation function's signature in order to pass all the live values during an OSR transition.
As a result, the continuation function might not be of type $T$.
Enabling any type in the ic stub is not a viable solution, i.e., it requires too much work and goes against the type policies enforced by the RJIT framework. 
We therefore have to come up with an alternate solution.\\

The next sections present the solutions implemented in RJIT to mitigate the OSR Kit limitations in the deoptimization case, while answering the above challenges particular to the RJIT framework.\\

\subsection{Reducing the number of compilation}

Transformations that act on R semantics have to be performed on a non-instrumented LLVM IR.
As explained before, obtaining such IR might be harder than expected. 
There are three cases to distinguish:
\begin{enumerate}
    \item The function was never compiled before.\label{toCompile}
    \item The function was compiled, but not instrumented.\label{best} 
    \item The function was compiled and instrumented.\label{worst}
\end{enumerate}

For case \ref{toCompile}, the only option is to compile the function, and extract the generated IR before the final instrumentation.
Case \ref{best} is the best scenario.
The LLVM IR can simply be extracted from the SEXP function, cloned, and used for the transformations.
This case happens when the function was compiled during the current compilation unit, i.e., the function is part of the current module and was generated using the current instance of the compiler, on which the \textit{jitAll} function was not called yet.
Case \ref{worst} is the worst scenario.
The non-instrumented LLVM IR no longer exists.
The naive solution is to recompile the function from scratch, which seems very expensive.\\

The OSR Handler enables to record non-instrumented IRs that it encounters.
This singleton provides a special function, called \textit{getFreshIR}, that takes as parameter a SEXP closure and a compiler instance, and returns a function SEXP (see signature Figure REF).
The OSR Handler extracts the function from the closure, and checks its type.
Depending on the type of the function, it selects the less costly solution to get the non-instrumented IR.\\

If the function is not a NATIVESXP, i.e., if it is not compiled, the OSR Handler invokes the compiler on the function, and creates two clones of the resulting LLVM IR.
One clone is stored in the \textit{base versions} map, a map from a SEXP closure to a NATIVESXP function SEXP.
The stored clone does not belong to the current module and will not be instrumented if the \textit{jitAll} function is called.
One should note that, in order to save time during execution, the OSRHandler does not fix the IR for the version stored in the map.
The other clone is wrapped into a copy of the original function SEXP and returned to the user.\\

Calling \textit{getFreshIR} on a function that was not previously compiled can be viewed as an ahead of time (AOT) compilation strategy.
RJIT is a JIT compiler, i.e., functions are compiled just before being executed.
Using the \textit{getFreshIR} can force the function's compilation to happen at anytime.
As a small optimization, the OSR Handler sets the result of the compilation inside the corresponding closure.
By doing so, we ensure that a subsequent call to this function will already benefit from the compiled version, and will not trigger a useless call to the JIT.\\

\includecode{Code/getFreshIR.h}
PROBLEM REFORMULATE\\

If the function is a NATIVESXP, the OSR Handler looks for the closure inside the base version map. 
If the map contains an entry for the closure, it simply clones the IR, encloses it in a copy of the function SEXP and returns it.
If the function is not in the base version map, the OSR Handler extracts the function's module.
This can be easily done by accessing the function's LLVM parent.
If the module is the same as the compiler's module, according to RJIT's behavior, we can assume that the IR is not instrumented.
RJIT creates one module per compiler instance, and calls the \textit{jitAll} function just before destroying the execution engine and the compiler instance, and returning to the R interpreter.
The OSR Handler therefore adds an entry for the closure to the base versions map, by simply cloning the function's IR. 
It then creates a second clone of the non-instrumented IR, creates a new SEXP containing the clone, and returns it.\\

If the compiled function's module is not the same as the compiler's one, the OSR Handler concludes that the LLVM IR of the function is instrumented.
As a result, it checks if a non-instrumented IR was registered in the \textit{base versions} map.
If such an entry exists, the OSR Handler constructs a copy of the function SEXP that contains a clone of the non-instrumented function and returns it.
Otherwise, the OSR Handler has no choice but to recompile the function, add an entry of the resulting non instrumented IR in the base version map, and construct a function SEXP to return to the user, as explained before.\\

\includecode{Code/baseVersion.h}

The non-instrumented IR obtained through the \textit{getFreshIR} corresponds to the result of a compilation of the original AST of the function, i.e., no transformation has been applied on this IR. 
In other words, all the transformations performed while compiling the function the first time are not reflected on the stored IR. 
This is a choice that we made in order to keep different optimization passes independent.
In some cases, that solution implies that we have to re-do the same transformation on the same function several times.
For example, consider a function $A$ that calls $B$ several times.
An inliner would inline every call sites in A.
The inliner could further decide to inline all the calls inside $B$'s body.
In that case, if the inliner relies on the OSR Handler to get $B$'s IR, it will have to run on each separate clone to inline the call sites they contain, therefore performing the same transformations multiple times.
On the other hand, if the inliner's transformations depend on the location of the call to $B$, it is guaranteed to obtain fresh and independent IRs through the OSR Handler.\\

A cloned function that needs to be fully compiled has to be fixed.
As explained in Section \ref{additionalchallenges}, the IR of a cloned function has incorrect attributes instrumentation, incorrect arguments to its ic stub calls, and might have missing targets in its call instructions, i.e., the callee has not been declared in the module.
A cloned function also needs to be added to the compilation module in order to be fully compiled.
The OSR Handler provides utilities functions to help the user fix the LLVM IR of cloned functions in RJIT. 
The \textit{resetSafepoints} function takes care of adding the proper safepoints and patchpoints to the IR, as well as fixing the arguments of ic stubs.
It further verifies that every callee in call instructions has been declared in the module. 
If that is not the case, it adds the proper function definition to the module.
The OSR Handler provides another function, \textit{addSexpToModule}, that enables to add a function to the current compilation module.
Moreover, a function needs to be wrapped inside a function SEXP before being returned to the GNUR interpreter.
The OSR Handler provides a \textit{cloneSexp} function that enables to create the proper SEXP for a function. 
Finally, in order to link the native code generated for an LLVM IR, the compilation engine needs to record a mapping between an LLVM IR and a SEXP. 
The \textit{addRelocation} function enables to do that for a clone SEXP and the cloned function it contains.
These utilities functions are divided so that each of them provides a single, fully contained, functionality. 
It enables the user to have a fine-grained control over the LLVM IR clones, and only perform the modifications that he desires.\\

\includecode{Code/utilities.h}

\subsection{Simplifying the OSR exit insertion}

Section \ref{osrkitlimitations} explains our choice to use resolved OSR from the OSR Kit\cite{OSRKit} library.
The resolved OSR API is provided in Section \ref{describeOSRKit}.\\

RJIT imposes additional constraints on the OSR exits. 
The continuation instruction needs to be the exact match of the from instruction.
In other words, considering the \textit{insertResolvedOSR}, the \textit{lPad} argument is the instruction in the continuation function that corresponds to the \textit{src} argument in the from function.
The OSR Kit library provides some flexibility in the choice of these two instructions. 
This flexibility is not required in RJIT, and is removed to ensure the correctness of the implementation.\\

The OSR Handler provides a clone function that clones an LLVM function, adds it to the same module as the original function, fixes the ic stubs attributes, and automatically registers a StateMap between the original and cloned function in its \textit{statemaps}, a map from a pair of LLVM function pointers to a StateMap (see Figure REF).\\
\includecode{Code/statemaps.h}

The cloned function can be called to generate what is called, in RJIT, the \textit{toInstrument} function.
The \textit{toInstrument} function is the argument passed as the continuation function to the \textit{insertResolvedOSR} function.
As explained in Section \ref{osrkitlimitations}, the OSR Kit library will clone the \textit{toInstrument} function to generate the continuation.
It relies on a StateMap that is passed as argument to the function call. 
Thanks to the OSR Handler statemaps, this argument can be omitted and automatically retrieved.\\

One might notice that the \textit{toInstrument} function was added to the module, and will therefore be fully compiled when the \textit{jitAll} function will be called.
As explained in Section \ref{additionalchallenges}, the continuation function's ic stub calls cannot use the continuation function's pointer as argument for the caller. 
The \textit{toInstrument} function's address is therefore used instead.
The \textit{toInstrument} function has another important role that is detailed in the next section.\\

The OSR Handler provides a function, called \textit{insertOSRExit}, that provides the simplified interface described in this section.
It further provides a default OSR configuration.
The function's prototype is provided in Figure REF.
The last argument is described in Section \ref{improvingexits}.\\

\includecode{Code/insertOSRExit.h}

\subsection{Improving the exits}\label{improvingexits}
%OSR kit not well suited for osr exits. 
%The call stub when executed will go replace itself in the toInstrument function, not in the stub.
%Hence compensation 

The continuation function mechanism does not fit the OSR exits well.
As explained in Section \ref{osrkitlimitations}, once an OSR exit is fired, it is likely to be fired in subsequent calls.
Triggering an OSR exit has a cost that might not be negligible.
The OSR Kit\cite{OSRKit} library does not provide any mechanism to improve that special case.
Furthermore, Section \ref{additionalchallenges} describes another problem with continuation functions.
If a continuation function contains an ic stub, the caller pointer argument has to be the \textit{toInstrument} address, since the continuation function does not have the correct signature.\\

The OSR Handler, via the \textit{insertOSRExit} method, enables to add a special compensation code at the beginning of the OSR ENTRY block in the continuation function, to answer these limitations.
The compensation code can be any valid vector of LLVM instructions.
This can be used to avoid triggering the OSR exit repeatedly once it has been fired a first time.
For example, the compensation vector can contain a special call to a C++ function, that takes as argument a unique id that identifies the continuation function. 
The C++ function then retrieves the toInstrument function, which is a valid, fully compiled version of the original function, and replaces the incorrect optimized function by the toInstrument version in the correct closure.
As a result, all subsequent calls to the same function will execute a version that was not optimized with the assumption that failed and triggered the OSR exit.
This enables to avoid the cost of triggering an exit upon every call to the function.\\

The compensation code in the OSR Handler does not rely on the OSR Kit compensation code mechanism.
The OSR Kit compensation code is used to fix the execution state, is associated to values transferred from one version to another, and is not meant to access the execution environment.
Since it pertains to a different goal, we decided to implement our own solution.
The instructions that enable to replace the function's version in the closure are left to the user. 
This provides some flexibility on how to decide whether or not to replace the current function.\\

In our example, we mentioned replacing the invalidated optimized version by the \textit{toInstrument} function.
This choice has several advantages.
First, since the \textit{toInstrument} was used to generate the continuation function, we know that it is correct and does not contain the invalidated transformations.
Second, when the continuation function's ic stubs are executed, the RJIT framework generates inlined cached version of the target function, and is supposed to replace the ic stubs by a call to this inlined cached functions.
For that purpose, it uses the ic stub argument that points to its caller.
This argument, in the continuation function, points to the \textit{toInstrument} function.
As a result, when the continuation function's ic stub are executed, the ic stubs in the toInstrument function are replaced by inlined cached functions, which are faster than the ic stubs.
This ensures that taking an OSR exit will have as little impact on the overall execution time as possible.
Furthermore, this mechanism ensures that executing a function twice, while experiencing an OSR exit in the first execution, will still have good performances compared to executing the non-optimized version the same number of times.\\

\subsection{Walkthrough a simple example}
In this Section, we provide a full example of a possible use of the OSR Handler's mechanisms.
It describes which functions are called, and how every element is used.
This example enables to show how the OSR Handler abstracts away all the different challenges that we described before, and enables to focus on the transformations performed on the input function.\\

Assume a transformation process \textit{TransP}, that speculatively inlines function calls inside the input function. 
\textit{TransP} receives as input the closure to optimize, and returns a closure containing the optimized version of the code.
Every function call inlined has a special OSR exit instrumentation to handle run time redefinition of the inlined functions.\\

\textit{TransP} gets the LLVM IR corresponding to the input function by calling the OSR Handler \textit{getFreshIR} function.
Call this copy the \textit{working copy}.
It then lists all the function calls performed inside the IR.
For each function call, sequentially, it gets a \textit{toInstrument} copy of the working copy by calling the OSR Handler clone function.
Each toInstrument is wrapped into its own SEXP, added to the module, has its IR corrected by the \textit{reseSafepoints} function, and is added to the relocations for the final native code to SEXP mapping.
It resolves the callee, and gets its LLVM IR via the OSR Handler \textit{getFreshIR}.
It then inlines the call in its working copy and calls the OSR Handler \textit{insertOSRExit} function.
It then calls the \textit{resetSafepoints} on the working copy to fix the IR, and returns its enclosing closure.
The working copy contains all the OSR exits needed for correctness, and all the continuation functions, and their corresponding \textit{toInstrument} versions are generated and part of the current module.\\


\section{Future work}
This Section describes solutions, design ideas, that were partially implemented but not fully explored during this master thesis.
RJIT being a young project, it lacks some supporting tools, e.g., a profiler or a code analyser, that could uncover new possibilities for the OSR implementation.
In the future, the development of the RJIT framework should enable to provide new incentives to implement more complex OSR-based speculative optimizations, or extend the OSR mechanisms.\\

\subsection{Transitive StateMaps}
%Trying to improve the field of possibilities
    %Chain of statemaps, can find what is in common in both of them
    %Why interesting? Will be able, if provides interface, to enlarge the scope of intermixing optimizations 
For the moment, in RJIT, the continuation function for an OSR exit is an instrumented version of the function that was used to generate the optimized version.
This is a requirement that results from the use, in OSR Kit\cite{OSRKit}, of StateMaps to insert the OSR instrumentation.
The from and the contination functions need to have a StateMap that records one-to-one mappings between their instructions.
In some cases, however, one might want to provide a different continuation function.
OSR Kit does not forbid this use of the library, but does not provide tools to make it easier.\\  

Consider the following chain of speculative optimizations performed on a base function $A$: 
$$A \xrightarrow[]{1} B \xrightarrow[]{2} C \xrightarrow[]{3} D \xrightarrow[]{4} E \xrightarrow[]{5} F \rightarrow ...$$
Each letter corresponds to a new version of the function. 
Each version is obtain by performing a speculative optimization of the previous version.
If an assumption fails, the continuation function will be an instrumented version of the base version on which the speculative optimization was performed.
For example, if the current version is $F$, and the OSR exit that corresponds to arrow 3 is taken, 
the function will exit to a continuation function that corresponds to version $C$.
One might want, instead, to OSR exit to $A$, or to generate on the fly a new continuation function that underwent all the transformations, except number 3. 
This might not always be possible, and requires to understand all the interactions between the different optimizations.
However, if all the transformation are independent, i.e., if none of them impacts on the others, this should be feasible.
We should be able to find a corresponding continuation instruction in any other version.\\

As a first step towards a more complex mapping between the from and the continuation function, we implemented a transitive StateMap constructor.
Suppose two fully generated StateMaps, $S_1$ and $S_2$, such that $S_1$ is a mapping between $A$ and $B$, and $S_2$ a mapping between $A$ and $C$.
Our transitive constructor enables to generate a new StateMap, $S_3$, that maps $B$ and $C$.\\

The transitive constructor can be extended to create StateMaps between different functions, generated by applying an arbitrary number of transformations on the same base function.
Consider the above chain of transformations.
The transitive StateMap constructor can be used to generate a transitive map between every pair of versions in the chain.
The resulting StateMap might not be complete, but it ensures that every instruction that is present in the base version $A$ and both versions that we want to map, e.g., $C$ and $F$, will be present in the resulting StateMap.\\

If we further extend the framework to automatically update the StateMap while modifying an LLVM IR, we might even be able to uncover new possible mappings between different versions, and lift the restriction on the transitive StateMaps, i.e., we could generate mappings between instructions, even if they do not appear in $A$ and in both versions $C$ and $F$.
This solution would be similar to the VARMAP in the Jikes RVM OSR implementation\cite{soman2006efficient}.
According to discussion we had with the OSR Kit designers, \citean{OSRKit} are working on a similar idea on the OSR Kit build compensation branch\cite{OSRKitGit}.
The idea is to provide basic LLVM transformation functions, e.g, addInstruction, removeInstruction etc., that automatically update the StateMap.\\

\subsection{On-the-fly compilation}

Generating the OSR exit continuation on the fly is hard in general. 
Section \ref{osrkitlimitations} details the challenges that this technique presents.
One option to simulate this behavior in RJIT is to save the LLVM IR of the base function in the OSR Handler, and remove it from the compilation unit.
In other words, we generate and save the IR, but we do not complete its compilation.
An open OSR is inserted in the optimized version. 
The $f_{stub}$ function is then responsible for identifying the correct continuation instruction inside the saved base IR, and completing its compilation using profiled guided techniques.\\

On-the-fly compilation of continuation functions requires to guarantee that a transitive StateMap can be generated between the from function, and the newly generated function.
RJIT is not mature enough to make this solution worth exploring.
First, there is no profiler, and hence, no profiled-guided transformation. 
Second, the kind of transformations that RJIT would like to perform are not yet known. 
It is too early in the implementation process to identify which optimizations might improve the execution of the function.
As a result, we did not put efforts in the implementation of this solution.\\

\subsection{A cleaner, more integrated way of getting a fresh IR}

The OSR Handler is a prototype, a proof of concept, to experiment OSR transitions in RJIT. 
As such, the choice was made, at the beginning of the implementation, to encapsulate everything related to the OSR deoptimization mechanism, e.g., the OSR Handler, the OSR Inliner (see Chapter \ref{Chapter5}), in their own classes.
In other words, everything added to the RJIT project was kept as independent to the main RJIT elements as possible.
This had the double benefit of 1) enforcing clear and stable interfaces that integrate well in the compilation flow, and 2) enabled to merge the master branch updates into the OSR branch easily.
Since the RJIT project is under active development, the second point was essential.
On the other hand, the OSR mechanisms could be greatly simplified, and made cleaner, if properly integrated inside the RJIT framework.\\

The OSR Handler, and more specifically the base versions map, could be, in the future, replaced by a careful instrumentation of the function's SEXP. 
For the moment, the TAG attribute of a compiled function's SEXP contains the LLVM IR used to generate the native function.
If the function went through the entire compilation flow, the LLVM IR is instrumented. 
In RJIT, this TAG LLVM IR is mostly used for debugging purposes.
In the future, a non-instrumented snapshot of the function's module could be stored instead.
When a transformation needs to be performed on an already compiled function, the snapshot can be loaded inside the current compilation module, and the non-instrumented LLVM IR of the function extracted from it.
This has the double benefit of easily providing an LLVM IR to work on, while removing any need to fix call instruction targets in the IR (see Section \ref{additionalchallenges} for more details on this matter).\\

For the moment, many parts of the implementation rely on the fact that a function's SEXP TAG contains the LLVM IR of the native code. 
As a result, this would not be a trivial change and needs the approval of the entire team.
However, after talking to the main developers, the decision was made to make this change happen in the future.\\